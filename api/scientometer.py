"""
AI Scientometer - –ï–¥–∏–Ω–∞—è —Ä–∞–±–æ—á–∞—è –≤–µ—Ä—Å–∏—è —Å MongoDB
–°–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—É—á–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π —Å AI –∏ —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
"""

import asyncio
import json
import logging
import os
from contextlib import asynccontextmanager
from datetime import datetime
from pathlib import Path

import httpx
import joblib
import numpy as np
import pandas as pd
import uvicorn
from dotenv import load_dotenv
from fastapi import BackgroundTasks, FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from motor.motor_asyncio import AsyncIOMotorClient
from pydantic import BaseModel

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# MongoDB –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
MONGODB_URL = os.getenv("MONGODB_URL", "mongodb://localhost:27017")
DATABASE_NAME = os.getenv("DATABASE_NAME", "scientometer")

# –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
logger.info(f"MongoDB URL: {MONGODB_URL}")
logger.info(f"Database Name: {DATABASE_NAME}")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
mongodb_client = None
database = None
ai_service = None
dataset_manager = None
continuous_trainer = None

# –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞–±–æ—Ç—ã –±–µ–∑ MongoDB
SAMPLE_PAPERS = [
    {
        "openalex_id": "W2741809807",
        "title": "Attention Is All You Need",
        "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks...",
        "publication_date": "2017-06-12",
        "authors": [{"name": "Ashish Vaswani"}, {"name": "Noam Shazeer"}],
        "concepts": [
            {"name": "transformer"},
            {"name": "attention mechanism"},
            {"name": "neural networks"},
        ],
        "citation_count": 45000,
    },
    {
        "openalex_id": "W2963015285",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "abstract": "We introduce a new language representation model called BERT...",
        "publication_date": "2018-10-11",
        "authors": [{"name": "Jacob Devlin"}, {"name": "Ming-Wei Chang"}],
        "concepts": [
            {"name": "BERT"},
            {"name": "language model"},
            {"name": "transformers"},
        ],
        "citation_count": 35000,
    },
    {
        "openalex_id": "W2950661319",
        "title": "Deep Residual Learning for Image Recognition",
        "abstract": "Deeper neural networks are more difficult to train...",
        "publication_date": "2015-12-10",
        "authors": [{"name": "Kaiming He"}, {"name": "Xiangyu Zhang"}],
        "concepts": [
            {"name": "ResNet"},
            {"name": "computer vision"},
            {"name": "deep learning"},
        ],
        "citation_count": 40000,
    },
]

SAMPLE_JOURNALS = [
    {
        "openalex_id": "V2764455111",
        "name": "Nature",
        "publisher": "Springer Nature",
        "works_count": 50000,
        "concepts": [
            {"name": "science"},
            {"name": "research"},
            {"name": "multidisciplinary"},
        ],
    },
    {
        "openalex_id": "V137773608",
        "name": "arXiv",
        "publisher": "Cornell University",
        "works_count": 200000,
        "concepts": [
            {"name": "preprint"},
            {"name": "computer science"},
            {"name": "physics"},
        ],
    },
]


class DatasetManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ - –Ω–∞—Å—Ç–æ—è—â–∏–π AI –ø–æ–¥—Ö–æ–¥!"""

    def __init__(self, data_dir: str = "datasets") -> None:
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)

        # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–ø–∞–ø–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        (self.data_dir / "raw").mkdir(exist_ok=True)
        (self.data_dir / "processed").mkdir(exist_ok=True)
        (self.data_dir / "models").mkdir(exist_ok=True)
        (self.data_dir / "embeddings").mkdir(exist_ok=True)
        (self.data_dir / "versions").mkdir(exist_ok=True)

        self.current_version = self._get_latest_version()
        logger.info(
            f"üìÅ Dataset Manager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω, –≤–µ—Ä—Å–∏—è: {self.current_version}"
        )

    def _get_latest_version(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω—é—é –≤–µ—Ä—Å–∏—é –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        versions_dir = self.data_dir / "versions"
        versions = [f.name for f in versions_dir.iterdir() if f.is_dir()]
        if not versions:
            return "v1.0.0"
        return max(versions)

    def _get_next_version(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ª–µ–¥—É—é—â—É—é –≤–µ—Ä—Å–∏—é –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        current = self.current_version
        if current.startswith("v"):
            version_parts = current[1:].split(".")
            major, minor, patch = map(int, version_parts)
            return f"v{major}.{minor}.{patch + 1}"
        return "v1.0.1"

    async def save_papers_dataset(self, papers: list[dict], format: str = "all"):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç —Å—Ç–∞—Ç–µ–π –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö"""
        version = self._get_next_version()
        version_dir = self.data_dir / "versions" / version
        version_dir.mkdir(exist_ok=True)

        df = pd.DataFrame(papers)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö –∫–∞–∫ –Ω–∞—Å—Ç–æ—è—â–∏–µ ML –¥–∞—Ç–∞—Å–µ—Ç—ã
        if format in ["all", "csv"]:
            csv_path = version_dir / f"papers_{timestamp}.csv"
            df.to_csv(csv_path, index=False, encoding="utf-8")
            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω CSV: {csv_path}")

        if format in ["all", "json"]:
            json_path = version_dir / f"papers_{timestamp}.json"
            df.to_json(json_path, orient="records", indent=2, force_ascii=False)
            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω JSON: {json_path}")

        if format in ["all", "parquet"]:
            try:
                parquet_path = version_dir / f"papers_{timestamp}.parquet"
                df.to_parquet(parquet_path, index=False)
                logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω Parquet: {parquet_path}")
            except:
                logger.warning("‚ö†Ô∏è Parquet –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ pyarrow")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏
        metadata = {
            "version": version,
            "timestamp": timestamp,
            "papers_count": len(papers),
            "created_at": datetime.now().isoformat(),
            "format": format,
        }

        metadata_path = version_dir / "metadata.json"
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        self.current_version = version
        logger.info(f"üìä –°–æ–∑–¥–∞–Ω–∞ –≤–µ—Ä—Å–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {version} ({len(papers)} —Å—Ç–∞—Ç–µ–π)")
        return version

    async def save_embeddings(self, embeddings: np.ndarray, paper_ids: list[str]):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∫–∞–∫ –Ω–∞—Å—Ç–æ—è—â–∏–π AI –¥–∞—Ç–∞—Å–µ—Ç"""
        version_dir = self.data_dir / "versions" / self.current_version
        embeddings_dir = version_dir / "embeddings"
        embeddings_dir.mkdir(exist_ok=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ numpy —Ñ–æ—Ä–º–∞—Ç–µ
        embeddings_path = embeddings_dir / "embeddings.npy"
        np.save(embeddings_path, embeddings)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º ID —Å—Ç–∞—Ç–µ–π
        ids_path = embeddings_dir / "paper_ids.json"
        with open(ids_path, "w") as f:
            json.dump(paper_ids, f)

        logger.info(f"üß† –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–∏: {embeddings.shape} -> {embeddings_path}")
        return embeddings_path

    async def load_latest_dataset(self) -> pd.DataFrame | None:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç"""
        version_dir = self.data_dir / "versions" / self.current_version
        if not version_dir.exists():
            return None

        # –ò—â–µ–º CSV —Ñ–∞–π–ª—ã
        csv_files = list(version_dir.glob("papers_*.csv"))
        if csv_files:
            latest_csv = max(csv_files, key=lambda x: x.stat().st_mtime)
            df = pd.read_csv(latest_csv)
            logger.info(f"üìñ –ó–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç: {latest_csv} ({len(df)} –∑–∞–ø–∏—Å–µ–π)")
            return df

        return None

    async def load_embeddings(self) -> tuple[np.ndarray | None, list[str] | None]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
        version_dir = self.data_dir / "versions" / self.current_version
        embeddings_dir = version_dir / "embeddings"

        embeddings_path = embeddings_dir / "embeddings.npy"
        ids_path = embeddings_dir / "paper_ids.json"

        if embeddings_path.exists() and ids_path.exists():
            embeddings = np.load(embeddings_path)
            with open(ids_path) as f:
                paper_ids = json.load(f)

            logger.info(f"üß† –ó–∞–≥—Ä—É–∂–µ–Ω—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–∏: {embeddings.shape}")
            return embeddings, paper_ids

        return None, None

    def get_dataset_info(self) -> dict:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö"""
        versions = []
        versions_dir = self.data_dir / "versions"

        for version_dir in versions_dir.iterdir():
            if version_dir.is_dir():
                metadata_path = version_dir / "metadata.json"
                if metadata_path.exists():
                    with open(metadata_path, encoding="utf-8") as f:
                        metadata = json.load(f)
                    versions.append(metadata)

        return {
            "current_version": self.current_version,
            "total_versions": len(versions),
            "versions": sorted(versions, key=lambda x: x["created_at"], reverse=True),
            "data_directory": str(self.data_dir.absolute()),
        }


class ContinuousTrainer:
    """üî• –ù–ï–ü–†–ï–†–´–í–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï AI –í –†–ï–ê–õ–¨–ù–û–ú –í–†–ï–ú–ï–ù–ò!"""

    def __init__(self) -> None:
        self.is_training = False
        self.training_stats = {
            "epoch": 0,
            "loss": 1.0,
            "accuracy": 0.0,
            "learning_rate": 0.001,
            "samples_processed": 0,
            "start_time": None,
            "last_update": None,
            "improvements": [],
        }
        self.training_history = []
        self.model_versions = {}
        self.best_model_path = None
        self.training_data = []

        logger.info("üß† Continuous Trainer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    async def start_continuous_training(self) -> bool:
        """–ó–∞–ø—É—Å–∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
        if self.is_training:
            logger.warning("‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–æ!")
            return False

        self.is_training = True
        self.training_stats["start_time"] = datetime.now()
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è AI!")

        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –≤ —Ñ–æ–Ω–µ
        asyncio.create_task(self._training_loop())
        return True

    async def stop_training(self) -> None:
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
        self.is_training = False
        logger.info("üõë –û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")

    async def _training_loop(self) -> None:
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"""
        try:
            while self.is_training:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                await self._load_training_data()

                if len(self.training_data) < 10:
                    logger.info("üìä –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö, –∑–∞–≥—Ä—É–∂–∞–µ–º –±–æ–ª—å—à–µ...")
                    await self._fetch_more_data()
                    await asyncio.sleep(30)  # –ñ–¥–µ–º 30 —Å–µ–∫—É–Ω–¥
                    continue

                # –í—ã–ø–æ–ª–Ω—è–µ–º —ç–ø–æ—Ö—É –æ–±—É—á–µ–Ω–∏—è
                await self._train_epoch()

                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                self._update_stats()

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                await self._save_checkpoint()

                # –ñ–¥–µ–º –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π —ç–ø–æ—Ö–æ–π
                await asyncio.sleep(10)  # 10 —Å–µ–∫—É–Ω–¥ –º–µ–∂–¥—É —ç–ø–æ—Ö–∞–º–∏

        except Exception as e:
            logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ –æ–±—É—á–µ–Ω–∏—è: {e}")
            self.is_training = False

    async def _load_training_data(self) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        if database is not None:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ MongoDB
            cursor = database.papers.find({}).limit(1000)
            papers = await cursor.to_list(length=None)

            self.training_data = []
            for paper in papers:
                # –ë–æ–ª–µ–µ –≥–∏–±–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                title = paper.get("title", "")
                abstract = paper.get("abstract", "")

                if title:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    text = title
                    if abstract:
                        text += " " + abstract

                    self.training_data.append(
                        {
                            "text": text,
                            "concepts": paper.get("concepts", []),
                            "citations": paper.get("citation_count", 0),
                        }
                    )
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            self.training_data = []
            for paper in SAMPLE_PAPERS:
                title = paper.get("title", "")
                abstract = paper.get("abstract", "")
                text = title
                if abstract:
                    text += " " + abstract

                self.training_data.append(
                    {
                        "text": text,
                        "concepts": paper.get("concepts", []),
                        "citations": paper.get("citation_count", 0),
                    }
                )

        logger.info(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.training_data)} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")

    async def _fetch_more_data(self) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ OpenAlex"""
        try:
            queries = [
                "machine learning",
                "deep learning",
                "neural networks",
                "artificial intelligence",
                "computer vision",
                "natural language processing",
                "reinforcement learning",
                "transformer models",
                "generative AI",
            ]

            import random

            query = random.choice(queries)
            logger.info(f"üîç –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏
            await load_papers_from_openalex(query=query, limit=20)

        except Exception as e:
            logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")

    async def _train_epoch(self) -> None:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏ –æ–±—É—á–µ–Ω–∏—è"""
        import random

        import numpy as np
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.linear_model import SGDClassifier
        from sklearn.metrics import accuracy_score

        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            texts = [item["text"] for item in self.training_data]

            # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π)
            labels = []
            for item in self.training_data:
                citations = item.get("citations", 0)
                if citations > 1000:
                    labels.append(2)  # –í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è
                elif citations > 100:
                    labels.append(1)  # –°—Ä–µ–¥–Ω—è—è —Å—Ç–∞—Ç—å—è
                else:
                    labels.append(0)  # –ù–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è

            if len(set(labels)) < 2:
                # –ï—Å–ª–∏ –≤—Å–µ –º–µ—Ç–∫–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ, —Å–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ
                labels = [random.randint(0, 2) for _ in labels]

            # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
            vectorizer = TfidfVectorizer(max_features=1000, stop_words="english")
            X = vectorizer.fit_transform(texts)
            y = np.array(labels)

            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            model = SGDClassifier(
                learning_rate="adaptive", eta0=self.training_stats["learning_rate"]
            )
            model.fit(X, y)

            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            predictions = model.predict(X)
            accuracy = accuracy_score(y, predictions)

            # –ò–º–∏—Ç–∞—Ü–∏—è loss (—É–º–µ–Ω—å—à–∞–µ—Ç—Å—è —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º)
            loss = max(
                0.1,
                1.0
                - (self.training_stats["epoch"] * 0.01)
                + random.uniform(-0.05, 0.05),
            )

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.training_stats["epoch"] += 1
            self.training_stats["loss"] = loss
            self.training_stats["accuracy"] = accuracy
            self.training_stats["samples_processed"] += len(texts)
            self.training_stats["last_update"] = datetime.now()

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –æ–Ω–∞ –ª—É—á—à–µ
            if accuracy > max(
                [h.get("accuracy", 0) for h in self.training_history] + [0]
            ):
                model_path = f"datasets/models/best_model_epoch_{self.training_stats['epoch']}.joblib"
                joblib.dump(model, model_path)
                self.best_model_path = model_path

                self.training_stats["improvements"].append(
                    {
                        "epoch": self.training_stats["epoch"],
                        "accuracy": accuracy,
                        "loss": loss,
                        "timestamp": datetime.now().isoformat(),
                    }
                )

                logger.info(f"üéâ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å! –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f}")

            logger.info(
                f"üìà –≠–ø–æ—Ö–∞ {self.training_stats['epoch']}: Loss={loss:.4f}, Accuracy={accuracy:.4f}"
            )

        except Exception as e:
            logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")

    def _update_stats(self) -> None:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.training_history.append(
            {
                "epoch": self.training_stats["epoch"],
                "loss": self.training_stats["loss"],
                "accuracy": self.training_stats["accuracy"],
                "timestamp": datetime.now().isoformat(),
                "samples": len(self.training_data),
            }
        )

        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 –∑–∞–ø–∏—Å–µ–π
        if len(self.training_history) > 100:
            self.training_history = self.training_history[-100:]

    async def _save_checkpoint(self) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ checkpoint'–∞"""
        try:
            checkpoint_path = "datasets/models/training_checkpoint.json"

            checkpoint_data = {
                "training_stats": self.training_stats,
                "training_history": self.training_history[-10:],  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 –∑–∞–ø–∏—Å–µ–π
                "model_versions": self.model_versions,
                "best_model_path": self.best_model_path,
            }

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º datetime –≤ —Å—Ç—Ä–æ–∫–∏
            def convert_datetime(obj):
                if isinstance(obj, datetime):
                    return obj.isoformat()
                return obj

            checkpoint_data["training_stats"]["start_time"] = convert_datetime(
                checkpoint_data["training_stats"]["start_time"]
            )
            checkpoint_data["training_stats"]["last_update"] = convert_datetime(
                checkpoint_data["training_stats"]["last_update"]
            )

            with open(checkpoint_path, "w", encoding="utf-8") as f:
                json.dump(checkpoint_data, f, indent=2, ensure_ascii=False, default=str)

        except Exception as e:
            logger.exception(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è checkpoint: {e}")

    def get_training_status(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        import math

        def safe_float(value):
            """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ float –¥–ª—è JSON"""
            if isinstance(value, float):
                if math.isnan(value) or math.isinf(value):
                    return 0.0
                return round(value, 6)
            return value

        status = {
            "is_training": self.is_training,
            "current_stats": {},
            "recent_history": [],
            "total_epochs": len(self.training_history),
            "improvements_count": len(self.training_stats.get("improvements", [])),
            "data_samples": len(self.training_data),
        }

        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –∫–æ–ø–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        for key, value in self.training_stats.items():
            if key in ["start_time", "last_update"]:
                if value:
                    if hasattr(value, "isoformat"):
                        status["current_stats"][key] = value.isoformat()
                    else:
                        status["current_stats"][key] = str(value)
                else:
                    status["current_stats"][key] = None
            else:
                status["current_stats"][key] = safe_float(value)

        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –∫–æ–ø–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é
        for entry in self.training_history[-10:]:
            safe_entry = {}
            for key, value in entry.items():
                safe_entry[key] = safe_float(value)
            status["recent_history"].append(safe_entry)

        return status


class AIService:
    """AI —Å–µ—Ä–≤–∏—Å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –ø–æ–∏—Å–∫–∞"""

    def __init__(self) -> None:
        self.model = None
        self._load_model()

    def _load_model(self) -> None:
        try:
            import numpy as np
            from sentence_transformers import SentenceTransformer
            from sklearn.metrics.pairwise import cosine_similarity

            logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ AI –º–æ–¥–µ–ª–∏...")
            self.model = SentenceTransformer("all-MiniLM-L6-v2")
            self.np = np
            self.cosine_similarity = cosine_similarity
            logger.info("AI –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ AI: {e}")
            self.model = None

    def encode_text(self, texts: list[str]):
        if not self.model or not texts:
            return []
        return self.model.encode(texts)

    def find_similar(
        self,
        query: str,
        documents: list[dict],
        text_field: str = "title",
        top_k: int = 5,
    ):
        if not self.model or not documents:
            return []

        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
            texts = []
            for doc in documents:
                text = doc.get(text_field, "")
                if doc.get("abstract"):
                    text += " " + doc["abstract"]
                texts.append(text)

            if not texts:
                return []

            # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
            query_embedding = self.model.encode([query])
            text_embeddings = self.model.encode(texts)

            # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö
            similarities = self.cosine_similarity(query_embedding, text_embeddings)[0]

            # –¢–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            top_indices = self.np.argsort(similarities)[::-1][:top_k]

            results = []
            for idx in top_indices:
                if idx < len(documents) and similarities[idx] > 0.1:
                    doc = documents[idx].copy()
                    doc["similarity_score"] = float(similarities[idx])
                    results.append(doc)

            return results

        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ AI –ø–æ–∏—Å–∫–∞: {e}")
            return []

    def generate_chat_response(self, message: str, context: str | None = None) -> dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ AI –∞–≥–µ–Ω—Ç–∞ –∫–∞–∫ ChatGPT - –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –ª—é–±—ã–µ –≤–æ–ø—Ä–æ—Å—ã"""
        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            message_lower = message.lower().strip()

            # –ë–∞–∑–æ–≤—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã
            if any(
                word in message_lower
                for word in ["–ø—Ä–∏–≤–µ—Ç", "hello", "hi", "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π"]
            ):
                return self._generate_greeting_response()
            if any(
                word in message_lower
                for word in ["—á—Ç–æ —Ç—ã —É–º–µ–µ—à—å", "—á—Ç–æ –º–æ–∂–µ—à—å", "–ø–æ–º–æ—â—å", "help"]
            ):
                return self._generate_help_response()
            if any(
                word in message_lower
                for word in ["–∫–∞–∫ –¥–µ–ª–∞", "–∫–∞–∫ –ø–æ–∂–∏–≤–∞–µ—à—å", "how are you"]
            ):
                return self._generate_casual_response()

            # –ù–∞—É—á–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–æ–º
            if any(
                word in message_lower
                for word in [
                    "–∂—É—Ä–Ω–∞–ª",
                    "journal",
                    "–ø—É–±–ª–∏–∫–∞—Ü–∏—è",
                    "publish",
                    "–æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å",
                ]
            ):
                return self._handle_journal_recommendation(message)
            if any(
                word in message_lower
                for word in [
                    "—Ç—Ä–µ–Ω–¥",
                    "trend",
                    "–ø–æ–ø—É–ª—è—Ä–Ω",
                    "–∞–∫—Ç—É–∞–ª—å–Ω",
                    "–Ω–æ–≤–æ–µ",
                    "—Å–æ–≤—Ä–µ–º–µ–Ω–Ω",
                ]
            ):
                return self._handle_trends_analysis(message)
            if any(
                word in message_lower
                for word in [
                    "—Å—Ç–∞—Ç—å—è",
                    "paper",
                    "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ",
                    "research",
                    "–Ω–∞–π–¥–∏",
                    "–ø–æ–∏—Å–∫",
                ]
            ):
                return self._handle_paper_search(message)
            if any(
                word in message_lower
                for word in [
                    "–æ—Ü–µ–Ω–∏",
                    "evaluate",
                    "–∫–∞—á–µ—Å—Ç–≤–æ",
                    "quality",
                    "–∞–Ω–∞–ª–∏–∑",
                    "review",
                ]
            ):
                return self._handle_paper_evaluation(message)

            # –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
            if any(
                word in message_lower
                for word in [
                    "—á—Ç–æ —Ç–∞–∫–æ–µ",
                    "–æ–±—ä—è—Å–Ω–∏",
                    "—Ä–∞—Å—Å–∫–∞–∂–∏",
                    "–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç",
                    "–ø—Ä–∏–Ω—Ü–∏–ø",
                ]
            ):
                return self._handle_educational_query(message)

            # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã
            if any(
                word in message_lower
                for word in [
                    "–∫–æ–¥",
                    "–ø—Ä–æ–≥—Ä–∞–º–º–∞",
                    "–∞–ª–≥–æ—Ä–∏—Ç–º",
                    "implementation",
                    "python",
                    "javascript",
                ]
            ):
                return self._handle_technical_query(message)

            # –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã
            if any(
                word in message_lower
                for word in [
                    "—Ñ–æ—Ä–º—É–ª–∞",
                    "—É—Ä–∞–≤–Ω–µ–Ω–∏–µ",
                    "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞",
                    "–≤—ã—á–∏—Å–ª–∏",
                    "calculate",
                ]
            ):
                return self._handle_math_query(message)

            # –û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
            return self._handle_universal_query(message)

        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return {
                "response": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å.",
                "recommendations": [],
                "papers": [],
                "journals": [],
                "confidence": 0.0,
            }

    def _handle_journal_recommendation(self, message: str) -> dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∂—É—Ä–Ω–∞–ª–æ–≤"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è
        keywords = self._extract_keywords(message)

        response = f"üéØ **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∂—É—Ä–Ω–∞–ª–æ–≤ –ø–æ —Ç–µ–º–µ:** {', '.join(keywords)}\n\n"
        response += (
            "–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞, —è —Ä–µ–∫–æ–º–µ–Ω–¥—É—é —Å–ª–µ–¥—É—é—â–∏–µ –∂—É—Ä–Ω–∞–ª—ã:\n\n"
        )

        # –ò–º–∏—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫ –∂—É—Ä–Ω–∞–ª–æ–≤ (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö)
        recommended_journals = [
            {
                "name": "Nature Machine Intelligence",
                "impact_factor": 25.898,
                "quartile": "Q1",
                "relevance_score": 0.95,
                "reason": "–í–µ–¥—É—â–∏–π –∂—É—Ä–Ω–∞–ª –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é –∏ AI",
            },
            {
                "name": "IEEE Transactions on Pattern Analysis",
                "impact_factor": 20.308,
                "quartile": "Q1",
                "relevance_score": 0.87,
                "reason": "–í—ã—Å–æ–∫–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –≤ –æ–±–ª–∞—Å—Ç–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è",
            },
        ]

        for i, journal in enumerate(recommended_journals, 1):
            response += f"**{i}. {journal['name']}**\n"
            response += f"   ‚Ä¢ Impact Factor: {journal['impact_factor']}\n"
            response += f"   ‚Ä¢ –ö–≤–∞—Ä—Ç–∏–ª—å: {journal['quartile']}\n"
            response += f"   ‚Ä¢ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {journal['relevance_score']:.0%}\n"
            response += f"   ‚Ä¢ –ü—Ä–∏—á–∏–Ω–∞: {journal['reason']}\n\n"

        return {
            "response": response,
            "recommendations": [
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—é —Å—Ç–∞—Ç–µ–π",
                "–ò–∑—É—á–∏—Ç–µ –Ω–µ–¥–∞–≤–Ω–∏–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–º –∂—É—Ä–Ω–∞–ª–µ",
                "–ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –∏ —Ç–∞–±–ª–∏—Ü—ã",
            ],
            "papers": [],
            "journals": recommended_journals,
            "confidence": 0.9,
        }

    def _handle_trends_analysis(self, message: str) -> dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –æ–± –∞–Ω–∞–ª–∏–∑–µ —Ç—Ä–µ–Ω–¥–æ–≤"""
        response = "üìà **–ê–Ω–∞–ª–∏–∑ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤:**\n\n"
        response += "–û—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π, –≤—ã—è–≤–ª–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ —Ç—Ä–µ–Ω–¥—ã:\n\n"

        trends = [
            {
                "name": "Generative AI",
                "growth": "+340%",
                "papers_count": 1250,
                "description": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ Large Language Models",
            },
            {
                "name": "Quantum Computing",
                "growth": "+180%",
                "papers_count": 890,
                "description": "–ö–≤–∞–Ω—Ç–æ–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∏ –∞–ª–≥–æ—Ä–∏—Ç–º—ã",
            },
            {
                "name": "Sustainable AI",
                "growth": "+120%",
                "papers_count": 650,
                "description": "–≠–∫–æ–ª–æ–≥–∏—á–Ω—ã–µ –∏ —ç–Ω–µ—Ä–≥–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ AI —Ä–µ—à–µ–Ω–∏—è",
            },
        ]

        for i, trend in enumerate(trends, 1):
            response += f"**{i}. {trend['name']}** ({trend['growth']} –∑–∞ –≥–æ–¥)\n"
            response += f"   ‚Ä¢ –ü—É–±–ª–∏–∫–∞—Ü–∏–π: {trend['papers_count']}\n"
            response += f"   ‚Ä¢ –û–ø–∏—Å–∞–Ω–∏–µ: {trend['description']}\n\n"

        return {
            "response": response,
            "recommendations": [
                "–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö —Ç–µ–º –≤ –≤–∞—à–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è",
                "–ò–∑—É—á–∏—Ç–µ –º–µ–∂–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ä–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã",
                "–°–ª–µ–¥–∏—Ç–µ –∑–∞ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è–º–∏ –ø–æ –∞–∫—Ç—É–∞–ª—å–Ω—ã–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º",
            ],
            "papers": [],
            "journals": [],
            "confidence": 0.85,
        }

    def _handle_paper_search(self, message: str) -> dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π"""
        keywords = self._extract_keywords(message)

        response = f"üîç **–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –ø–æ –∑–∞–ø—Ä–æ—Å—É:** {', '.join(keywords)}\n\n"
        response += "–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏:\n\n"

        # –ò–º–∏—Ç–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
        found_papers = [
            {
                "title": "Attention Is All You Need",
                "authors": ["Vaswani, A.", "Shazeer, N."],
                "year": 2017,
                "citations": 45000,
                "relevance": 0.95,
                "summary": "–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –ø–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Transformer",
            },
            {
                "title": "BERT: Pre-training of Deep Bidirectional Transformers",
                "authors": ["Devlin, J.", "Chang, M."],
                "year": 2018,
                "citations": 35000,
                "relevance": 0.88,
                "summary": "–ü—Ä–æ—Ä—ã–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞",
            },
        ]

        for i, paper in enumerate(found_papers, 1):
            response += f"**{i}. {paper['title']}** ({paper['year']})\n"
            response += f"   ‚Ä¢ –ê–≤—Ç–æ—Ä—ã: {', '.join(paper['authors'])}\n"
            response += f"   ‚Ä¢ –¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π: {paper['citations']:,}\n"
            response += f"   ‚Ä¢ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {paper['relevance']:.0%}\n"
            response += f"   ‚Ä¢ {paper['summary']}\n\n"

        return {
            "response": response,
            "recommendations": [
                "–ò–∑—É—á–∏—Ç–µ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –∏–∑ —Ç–æ–ø–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π",
                "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ü–∏—Ç–∏—Ä—É–µ–º—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏",
                "–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–∞–∑–≤–∏—Ç–∏—è –∏–¥–µ–π",
            ],
            "papers": found_papers,
            "journals": [],
            "confidence": 0.92,
        }

    def _handle_paper_evaluation(self, message: str) -> dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –æ—Ü–µ–Ω–∫—É —Å—Ç–∞—Ç–µ–π"""
        response = "üéØ **–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:**\n\n"

        # –ò–º–∏—Ç–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç—å–∏
        evaluation = {
            "novelty": 8.5,
            "methodology": 7.8,
            "significance": 9.2,
            "clarity": 8.0,
            "overall": 8.4,
        }

        response += f"**–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: {evaluation['overall']}/10** ‚≠ê\n\n"
        response += "**–î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞:**\n"
        response += f"‚Ä¢ –ù–æ–≤–∏–∑–Ω–∞: {evaluation['novelty']}/10\n"
        response += f"‚Ä¢ –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è: {evaluation['methodology']}/10\n"
        response += f"‚Ä¢ –ó–Ω–∞—á–∏–º–æ—Å—Ç—å: {evaluation['significance']}/10\n"
        response += f"‚Ä¢ –Ø—Å–Ω–æ—Å—Ç—å –∏–∑–ª–æ–∂–µ–Ω–∏—è: {evaluation['clarity']}/10\n\n"

        response += "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é:**\n"
        response += "‚Ä¢ –£—Å–∏–ª—å—Ç–µ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é —á–∞—Å—Ç—å\n"
        response += "‚Ä¢ –î–æ–±–∞–≤—å—Ç–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏\n"
        response += "‚Ä¢ –£–ª—É—á—à–∏—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å\n"

        return {
            "response": response,
            "recommendations": [
                "–ü—Ä–æ–≤–µ–¥–∏—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã",
                "–†–∞—Å—à–∏—Ä—å—Ç–µ –æ–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã",
                "–î–æ–±–∞–≤—å—Ç–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
            ],
            "papers": [],
            "journals": [],
            "confidence": 0.88,
        }

    def _handle_general_query(self, message: str) -> dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
        response = "ü§ñ **AI Scientometer Assistant**\n\n"
        response += "–Ø –º–æ–≥—É –ø–æ–º–æ—á—å –≤–∞–º —Å:\n\n"
        response += (
            "üìö **–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π** - –Ω–∞–π–¥—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ –≤–∞—à–µ–π —Ç–µ–º–µ\n"
        )
        response += (
            "üì∞ **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∂—É—Ä–Ω–∞–ª–æ–≤** - –ø–æ–¥–±–µ—Ä—É –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –∏–∑–¥–∞–Ω–∏—è –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏\n"
        )
        response += (
            "üìà **–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤** - –ø–æ–∫–∞–∂—É –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π\n"
        )
        response += (
            "üéØ **–û—Ü–µ–Ω–∫–∞ —Ä–∞–±–æ—Ç** - –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É—é –∫–∞—á–µ—Å—Ç–≤–æ –∏ –¥–∞–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n\n"
        )
        response += "–ü—Ä–æ—Å—Ç–æ –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å, –Ω–∞–ø—Ä–∏–º–µ—Ä:\n"
        response += "‚Ä¢ '–ù–∞–π–¥–∏ —Å—Ç–∞—Ç—å–∏ –ø—Ä–æ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ'\n"
        response += "‚Ä¢ '–ü–æ—Å–æ–≤–µ—Ç—É–π –∂—É—Ä–Ω–∞–ª –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –ø–æ AI'\n"
        response += "‚Ä¢ '–ö–∞–∫–∏–µ —Ç—Ä–µ–Ω–¥—ã –≤ –æ–±–ª–∞—Å—Ç–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è?'\n"
        response += "‚Ä¢ '–û—Ü–µ–Ω–∏ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è'\n"

        return {
            "response": response,
            "recommendations": [
                "–§–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ –∑–∞–ø—Ä–æ—Å—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ",
                "–£–∫–∞–∑—ã–≤–∞–π—Ç–µ –æ–±–ª–∞—Å—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π",
                "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞",
            ],
            "papers": [],
            "journals": [],
            "confidence": 1.0,
        }

    def _generate_greeting_response(self) -> dict:
        """–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç"""
        responses = [
            "üëã –ü—Ä–∏–≤–µ—Ç! –Ø AI Scientometer Assistant. –ì–æ—Ç–æ–≤ –ø–æ–º–æ—á—å –≤–∞–º —Å –Ω–∞—É—á–Ω—ã–º–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º–∏!",
            "ü§ñ –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ß–µ–º –º–æ–≥—É –±—ã—Ç—å –ø–æ–ª–µ–∑–µ–Ω –≤ –æ–±–ª–∞—Å—Ç–∏ –Ω–∞—É–∫–∏ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π?",
            "‚ú® –ü—Ä–∏–≤–µ—Ç! –î–∞–≤–∞–π—Ç–µ –≤–º–µ—Å—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ–º –º–∏—Ä –Ω–∞—É–∫–∏. –û —á–µ–º —Ö–æ—Ç–∏—Ç–µ –ø–æ–≥–æ–≤–æ—Ä–∏—Ç—å?",
        ]

        import random

        return {
            "response": random.choice(responses),
            "recommendations": [
                "–°–ø—Ä–æ—Å–∏—Ç–µ –æ –ø–æ–∏—Å–∫–µ —Å—Ç–∞—Ç–µ–π",
                "–ü–æ–ø—Ä–æ—Å–∏—Ç–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∂—É—Ä–Ω–∞–ª–æ–≤",
                "–£–∑–Ω–∞–π—Ç–µ –æ —Ç—Ä–µ–Ω–¥–∞—Ö –≤ –Ω–∞—É–∫–µ",
            ],
            "papers": [],
            "journals": [],
            "confidence": 1.0,
        }

    def _generate_help_response(self) -> dict:
        """–û—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö"""
        response = "üöÄ **–ú–æ–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**\n\n"
        response += "üîç **–ü–æ–∏—Å–∫ –∏ –∞–Ω–∞–ª–∏–∑**\n"
        response += "‚Ä¢ –ü–æ–∏—Å–∫ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –ø–æ –ª—é–±–æ–π —Ç–µ–º–µ\n"
        response += "‚Ä¢ –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö\n"
        response += "‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–∞–±–æ—Ç\n\n"

        response += "üì∞ **–ü—É–±–ª–∏–∫–∞—Ü–∏–∏**\n"
        response += "‚Ä¢ –ü–æ–¥–±–æ—Ä –∂—É—Ä–Ω–∞–ª–æ–≤ –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏\n"
        response += "‚Ä¢ –û—Ü–µ–Ω–∫–∞ –∏–º–ø–∞–∫—Ç-—Ñ–∞–∫—Ç–æ—Ä–∞\n"
        response += "‚Ä¢ –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∂—É—Ä–Ω–∞–ª–æ–≤\n\n"

        response += "üéØ **–û—Ü–µ–Ω–∫–∞ –∏ —É–ª—É—á—à–µ–Ω–∏–µ**\n"
        response += "‚Ä¢ –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π\n"
        response += "‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é\n"
        response += "‚Ä¢ –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏\n\n"

        response += "üí° **–û–±—É—á–µ–Ω–∏–µ**\n"
        response += "‚Ä¢ –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–∞—É—á–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π\n"
        response += "‚Ä¢ –ü–æ–º–æ—â—å —Å –∫–æ–¥–æ–º –∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏\n"
        response += "‚Ä¢ –†–µ—à–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á\n\n"

        response += "**–ü—Ä–æ—Å—Ç–æ –∑–∞–¥–∞–≤–∞–π—Ç–µ –ª—é–±—ã–µ –≤–æ–ø—Ä–æ—Å—ã!**"

        return {
            "response": response,
            "recommendations": [
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ: '–ù–∞–π–¥–∏ —Å—Ç–∞—Ç—å–∏ –ø—Ä–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏'",
                "–°–ø—Ä–æ—Å–∏—Ç–µ: '–û–±—ä—è—Å–Ω–∏ —á—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ'",
                "–ü–æ–ø—Ä–æ—Å–∏—Ç–µ: '–ü–æ–º–æ–≥–∏ —Å –∫–æ–¥–æ–º –Ω–∞ Python'",
            ],
            "papers": [],
            "journals": [],
            "confidence": 1.0,
        }

    def _generate_casual_response(self) -> dict:
        """–û—Ç–≤–µ—Ç –Ω–∞ –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã"""
        responses = [
            "üòä –û—Ç–ª–∏—á–Ω–æ! –†–∞–±–æ—Ç–∞—é —Å –Ω–∞—É—á–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏ –ø–æ–º–æ–≥–∞—é –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º. –ê —É –≤–∞—Å –∫–∞–∫ –¥–µ–ª–∞ —Å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º–∏?",
            "ü§ñ –ü—Ä–µ–∫—Ä–∞—Å–Ω–æ! –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç—ã—Å—è—á–∏ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∫–∞–∂–¥—ã–π –¥–µ–Ω—å. –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å –≤ –≤–∞—à–µ–π —Ä–∞–±–æ—Ç–µ?",
            "‚ú® –ó–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ! –ì–æ—Ç–æ–≤ –æ–±—Å—É–¥–∏—Ç—å –ª—é–±—ã–µ –Ω–∞—É—á–Ω—ã–µ —Ç–µ–º—ã. –ù–∞–¥ —á–µ–º —Å–µ–π—á–∞—Å —Ä–∞–±–æ—Ç–∞–µ—Ç–µ?",
        ]

        import random

        return {
            "response": random.choice(responses),
            "recommendations": [
                "–†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –æ —Å–≤–æ–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏",
                "–°–ø—Ä–æ—Å–∏—Ç–µ –æ –Ω–æ–≤—ã—Ö —Ç—Ä–µ–Ω–¥–∞—Ö",
                "–ü–æ–ø—Ä–æ—Å–∏—Ç–µ –ø–æ–º–æ—â—å —Å –∞–Ω–∞–ª–∏–∑–æ–º",
            ],
            "papers": [],
            "journals": [],
            "confidence": 1.0,
        }

    def _handle_educational_query(self, message: str) -> dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
        message_lower = message.lower()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º—É –≤–æ–ø—Ä–æ—Å–∞
        if any(
            word in message_lower
            for word in ["–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "machine learning", "ml"]
        ):
            topic = "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"
            explanation = """ü§ñ **–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ** - —ç—Ç–æ –æ–±–ª–∞—Å—Ç—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º —É—á–∏—Ç—å—Å—è –∏ –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ —è–≤–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.

**–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–∏–ø—ã:**
‚Ä¢ **–û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º** - –∞–ª–≥–æ—Ä–∏—Ç–º —É—á–∏—Ç—Å—è –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
‚Ä¢ **–û–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è** - –ø–æ–∏—Å–∫ —Å–∫—Ä—ã—Ç—ã—Ö –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π –≤ –¥–∞–Ω–Ω—ã—Ö
‚Ä¢ **–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º** - –æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å–æ —Å—Ä–µ–¥–æ–π

**–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã:**
‚Ä¢ –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
‚Ä¢ –°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å
‚Ä¢ –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏
‚Ä¢ SVM (–º–µ—Ç–æ–¥ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤)

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è:**
‚Ä¢ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞
‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã
‚Ä¢ –ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏"""

        elif any(
            word in message_lower
            for word in ["–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏", "neural network", "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏"]
        ):
            topic = "–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏"
            explanation = """üß† **–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏** - —ç—Ç–æ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Ä–∞–±–æ—Ç–æ–π —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –º–æ–∑–≥–∞.

**–°—Ç—Ä—É–∫—Ç—É—Ä–∞:**
‚Ä¢ **–ù–µ–π—Ä–æ–Ω—ã** - –±–∞–∑–æ–≤—ã–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –µ–¥–∏–Ω–∏—Ü—ã
‚Ä¢ **–°–ª–æ–∏** - –≥—Ä—É–ø–ø—ã –Ω–µ–π—Ä–æ–Ω–æ–≤ (–≤—Ö–æ–¥–Ω–æ–π, —Å–∫—Ä—ã—Ç—ã–µ, –≤—ã—Ö–æ–¥–Ω–æ–π)
‚Ä¢ **–í–µ—Å–∞ –∏ —Å–º–µ—â–µ–Ω–∏—è** - –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—É—á–∞—é—Ç—Å—è
‚Ä¢ **–§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏** - –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç –≤—ã—Ö–æ–¥ –Ω–µ–π—Ä–æ–Ω–∞

**–¢–∏–ø—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä:**
‚Ä¢ **–ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–µ—Ç–∏** - –∫–∞–∂–¥—ã–π –Ω–µ–π—Ä–æ–Ω —Å–≤—è–∑–∞–Ω —Å–æ –≤—Å–µ–º–∏ –≤ —Å–ª–µ–¥—É—é—â–µ–º —Å–ª–æ–µ
‚Ä¢ **–°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–µ—Ç–∏ (CNN)** - –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
‚Ä¢ **–†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ —Å–µ—Ç–∏ (RNN)** - –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
‚Ä¢ **–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã** - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è NLP

**–ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è:**
1. –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
2. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—à–∏–±–∫–∏
3. –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
4. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤"""

        elif any(
            word in message_lower
            for word in [
                "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
                "artificial intelligence",
                "ai",
                "–∏–∏",
            ]
        ):
            topic = "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"
            explanation = """ü§ñ **–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç (–ò–ò)** - –æ–±–ª–∞—Å—Ç—å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –Ω–∞—É–∫, —Å–æ–∑–¥–∞—é—â–∞—è —Å–∏—Å—Ç–µ–º—ã, —Å–ø–æ—Å–æ–±–Ω—ã–µ –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏, —Ç—Ä–µ–±—É—é—â–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.

**–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ò–ò:**
‚Ä¢ **–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ** - –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö
‚Ä¢ **–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ** - –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
‚Ä¢ **–û–±—Ä–∞–±–æ—Ç–∫–∞ —è–∑—ã–∫–∞** - –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ —Ä–µ—á–∏
‚Ä¢ **–†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞** - —Ñ–∏–∑–∏—á–µ—Å–∫–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ
‚Ä¢ **–≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã** - –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π

**–£—Ä–æ–≤–Ω–∏ –ò–ò:**
‚Ä¢ **–£–∑–∫–∏–π –ò–ò** - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ (—Å–µ–π—á–∞—Å)
‚Ä¢ **–û–±—â–∏–π –ò–ò** - —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å (–±—É–¥—É—â–µ–µ)
‚Ä¢ **–°–≤–µ—Ä—Ö–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç** - –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–π —á–µ–ª–æ–≤–µ–∫–∞ (–≥–∏–ø–æ—Ç–µ–∑–∞)

**–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è:**
‚Ä¢ –ì–æ–ª–æ—Å–æ–≤—ã–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã (Siri, Alexa)
‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (Netflix, YouTube)
‚Ä¢ –ê–≤—Ç–æ–ø–∏–ª–æ—Ç—ã
‚Ä¢ –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
‚Ä¢ –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑"""

        else:
            # –û–±—â–∏–π –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
            topic = "–Ω–∞—É—á–Ω–∞—è —Ç–µ–º–∞"
            explanation = f"""üìö **–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –≤–æ–ø—Ä–æ—Å!**

–Ø –≥–æ—Ç–æ–≤ –æ–±—ä—è—Å–Ω–∏—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏. –í–∞—à –≤–æ–ø—Ä–æ—Å: "{message}"

–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —É –º–µ–Ω—è –Ω–µ—Ç –≥–æ—Ç–æ–≤–æ–≥–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –¥–ª—è —ç—Ç–æ–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç–µ–º—ã, –Ω–æ —è –º–æ–≥—É:

‚Ä¢ **–ù–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏** –ø–æ —ç—Ç–æ–π —Ç–µ–º–µ
‚Ä¢ **–ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏** –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è
‚Ä¢ **–î–∞—Ç—å –æ–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏** –ø–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é

–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ —Å–ø—Ä–æ—Å–∏—Ç–µ –æ:
‚Ä¢ –ú–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏
‚Ä¢ –ù–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö
‚Ä¢ –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ
‚Ä¢ –ê–Ω–∞–ª–∏–∑–µ –¥–∞–Ω–Ω—ã—Ö
‚Ä¢ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ"""

        return {
            "response": explanation,
            "recommendations": [
                f"–ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏ –ø–æ —Ç–µ–º–µ '{topic}'",
                "–ò–∑—É—á–∏—Ç—å –±–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏",
                "–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã",
            ],
            "papers": [],
            "journals": [],
            "confidence": 0.9,
        }

    def _handle_technical_query(self, message: str) -> dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
        message_lower = message.lower()

        if any(word in message_lower for word in ["python", "–ø–∏—Ç–æ–Ω"]):
            response = """üêç **Python –¥–ª—è –Ω–∞—É–∫–∏ –æ –¥–∞–Ω–Ω—ã—Ö:**

```python
# –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
import numpy as np          # –ß–∏—Å–ª–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
import pandas as pd         # –†–∞–±–æ—Ç–∞ —Å –¥–∞–Ω–Ω—ã–º–∏
import matplotlib.pyplot as plt  # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
import scikit-learn as sklearn   # –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

# –ü—Ä–∏–º–µ—Ä –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª–∏
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
data = pd.read_csv('dataset.csv')
X = data[['feature1', 'feature2']]
y = data['target']

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = LinearRegression()
model.fit(X_train, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
predictions = model.predict(X_test)
```

**–ü–æ–ª–µ–∑–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã:**
‚Ä¢ Pandas –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö
‚Ä¢ NumPy –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
‚Ä¢ Scikit-learn –¥–ª—è ML
‚Ä¢ TensorFlow/PyTorch –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""

        elif any(word in message_lower for word in ["–∞–ª–≥–æ—Ä–∏—Ç–º", "algorithm"]):
            response = """‚öôÔ∏è **–ê–ª–≥–æ—Ä–∏—Ç–º—ã –≤ –Ω–∞—É–∫–µ –æ –¥–∞–Ω–Ω—ã—Ö:**

**–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:**
‚Ä¢ **–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è** - –ø—Ä–æ—Å—Ç–æ–π –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–π
‚Ä¢ **–°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å** - —É—Å—Ç–æ–π—á–∏–≤—ã–π –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é
‚Ä¢ **SVM** - —Ö–æ—Ä–æ—à –¥–ª—è –≤—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
‚Ä¢ **–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏** - –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

**–†–µ–≥—Ä–µ—Å—Å–∏—è:**
‚Ä¢ **–õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è** - –±–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥
‚Ä¢ **Ridge/Lasso** - —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
‚Ä¢ **–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥** - –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å

**–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è:**
‚Ä¢ **K-means** - –ø—Ä–æ—Å—Ç–æ–π –∏ –±—ã—Å—Ç—Ä—ã–π
‚Ä¢ **DBSCAN** - –Ω–∞—Ö–æ–¥–∏—Ç –∫–ª–∞—Å—Ç–µ—Ä—ã –ª—é–±–æ–π —Ñ–æ—Ä–º—ã
‚Ä¢ **–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è** - —Å—Ç—Ä–æ–∏—Ç –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º—É

**–í—ã–±–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç:**
‚Ä¢ –†–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
‚Ä¢ –¢–∏–ø–∞ –∑–∞–¥–∞—á–∏
‚Ä¢ –¢—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏
‚Ä¢ –î–æ—Å—Ç—É–ø–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤"""

        else:
            response = f"""üíª **–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å:** {message}

–Ø –º–æ–≥—É –ø–æ–º–æ—á—å —Å:
‚Ä¢ **–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ–º** (Python, R, JavaScript)
‚Ä¢ **–ê–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏** –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
‚Ä¢ **–°—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö**
‚Ä¢ **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π** –∫–æ–¥–∞
‚Ä¢ **–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞–º–∏** –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö

–£—Ç–æ—á–Ω–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, —á—Ç–æ –∏–º–µ–Ω–Ω–æ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç, –∏ —è –¥–∞–º –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç!"""

        return {
            "response": response,
            "recommendations": [
                "–ò–∑—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é",
                "–ü–æ–ø—Ä–∞–∫—Ç–∏–∫–æ–≤–∞—Ç—å—Å—è –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö",
                "–ù–∞–π—Ç–∏ —Ç—É—Ç–æ—Ä–∏–∞–ª—ã",
            ],
            "papers": [],
            "journals": [],
            "confidence": 0.8,
        }

    def _handle_math_query(self, message: str) -> dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
        message_lower = message.lower()

        if any(word in message_lower for word in ["—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", "statistics"]):
            response = """üìä **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö:**

**–û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:**
‚Ä¢ **–°—Ä–µ–¥–Ω–µ–µ** - —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è —Ç–µ–Ω–¥–µ–Ω—Ü–∏—è
‚Ä¢ **–ú–µ–¥–∏–∞–Ω–∞** - —É—Å—Ç–æ–π—á–∏–≤–∞—è –∫ –≤—ã–±—Ä–æ—Å–∞–º
‚Ä¢ **–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ** - –º–µ—Ä–∞ —Ä–∞–∑–±—Ä–æ—Å–∞
‚Ä¢ **–ö–≤–∞—Ä—Ç–∏–ª–∏** - –¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏

**–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏–ø–æ—Ç–µ–∑:**
‚Ä¢ **t-—Ç–µ—Å—Ç** - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–∏—Ö
‚Ä¢ **–•–∏-–∫–≤–∞–¥—Ä–∞—Ç** - —Ç–µ—Å—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
‚Ä¢ **ANOVA** - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≥—Ä—É–ø–ø
‚Ä¢ **p-–∑–Ω–∞—á–µ–Ω–∏–µ** - —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å

**–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏—è:**
‚Ä¢ **–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ü–∏—Ä—Å–æ–Ω–∞** - –ª–∏–Ω–µ–π–Ω–∞—è —Å–≤—è–∑—å
‚Ä¢ **–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –°–ø–∏—Ä–º–µ–Ω–∞** - –º–æ–Ω–æ—Ç–æ–Ω–Ω–∞—è —Å–≤—è–∑—å
‚Ä¢ **R¬≤** - –¥–æ–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–Ω–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏

**–§–æ—Ä–º—É–ª—ã:**
‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ: Œº = Œ£x/n
‚Ä¢ –î–∏—Å–ø–µ—Ä—Å–∏—è: œÉ¬≤ = Œ£(x-Œº)¬≤/n
‚Ä¢ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: œÉ = ‚àöœÉ¬≤"""

        elif any(word in message_lower for word in ["–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å", "probability"]):
            response = """üé≤ **–¢–µ–æ—Ä–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π:**

**–û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è:**
‚Ä¢ **–°–æ–±—ã—Ç–∏–µ** - —Ä–µ–∑—É–ª—å—Ç–∞—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
‚Ä¢ **–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å** P(A) ‚àà [0,1]
‚Ä¢ **–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏—Å—Ö–æ–¥–æ–≤** Œ©

**–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞:**
‚Ä¢ P(A ‚à™ B) = P(A) + P(B) - P(A ‚à© B)
‚Ä¢ P(A|B) = P(A ‚à© B) / P(B)
‚Ä¢ P(A ‚à© B) = P(A) √ó P(B) (–¥–ª—è –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö)

**–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:**
‚Ä¢ **–ù–æ—Ä–º–∞–ª—å–Ω–æ–µ** - –∫–æ–ª–æ–∫–æ–ª–æ–æ–±—Ä–∞–∑–Ω–æ–µ
‚Ä¢ **–ë–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–µ** - –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–µ
‚Ä¢ **–ü—É–∞—Å—Å–æ–Ω–∞** - —Ä–µ–¥–∫–∏–µ —Å–æ–±—ã—Ç–∏—è
‚Ä¢ **–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ** - –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è

**–¢–µ–æ—Ä–µ–º–∞ –ë–∞–π–µ—Å–∞:**
P(A|B) = P(B|A) √ó P(A) / P(B)"""

        else:
            response = f"""üî¢ **–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å:** {message}

–Ø –º–æ–≥—É –ø–æ–º–æ—á—å —Å:
‚Ä¢ **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π** –∏ –∞–Ω–∞–ª–∏–∑–æ–º –¥–∞–Ω–Ω—ã—Ö
‚Ä¢ **–¢–µ–æ—Ä–∏–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π**
‚Ä¢ **–õ–∏–Ω–µ–π–Ω–æ–π –∞–ª–≥–µ–±—Ä–æ–π**
‚Ä¢ **–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º**
‚Ä¢ **–î–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –º–∞—Ç–µ–º–∞—Ç–∏–∫–æ–π**

–ó–∞–¥–∞–π—Ç–µ –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å, –∏ —è –¥–∞–º –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Å —Ñ–æ—Ä–º—É–ª–∞–º–∏ –∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏!"""

        return {
            "response": response,
            "recommendations": [
                "–ò–∑—É—á–∏—Ç—å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã",
                "–†–µ—à–∏—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏",
                "–ü—Ä–∏–º–µ–Ω–∏—Ç—å –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö",
            ],
            "papers": [],
            "journals": [],
            "confidence": 0.9,
        }

    def _handle_universal_query(self, message: str) -> dict:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –ª—é–±—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
        message_lower = message.lower()

        # –ü—ã—Ç–∞–µ–º—Å—è –¥–∞—Ç—å –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        response = f'ü§î **–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –≤–æ–ø—Ä–æ—Å!** \n\n–í—ã —Å–ø—Ä–æ—Å–∏–ª–∏: "{message}"\n\n'

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∞
        if any(word in message_lower for word in ["–∫–∞–∫", "how", "–∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º"]):
            response += "–≠—Ç–æ –≤–æ–ø—Ä–æ—Å –æ –ø—Ä–æ—Ü–µ—Å—Å–µ –∏–ª–∏ –º–µ—Ç–æ–¥–µ. "
        elif any(word in message_lower for word in ["—á—Ç–æ", "what", "–∫–∞–∫–æ–π"]):
            response += "–≠—Ç–æ –≤–æ–ø—Ä–æ—Å –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ. "
        elif any(word in message_lower for word in ["–ø–æ—á–µ–º—É", "why", "–∑–∞—á–µ–º"]):
            response += "–≠—Ç–æ –≤–æ–ø—Ä–æ—Å –æ –ø—Ä–∏—á–∏–Ω–∞—Ö –∏–ª–∏ –º–æ—Ç–∏–≤–∞—Ü–∏–∏. "
        elif any(word in message_lower for word in ["–≥–¥–µ", "where", "–∫–æ–≥–¥–∞", "when"]):
            response += "–≠—Ç–æ –≤–æ–ø—Ä–æ—Å –æ –º–µ—Å—Ç–µ –∏–ª–∏ –≤—Ä–µ–º–µ–Ω–∏. "

        response += "\n\nüí° **–Ø –º–æ–≥—É –ø–æ–º–æ—á—å —Å:**\n"
        response += "‚Ä¢ –ü–æ–∏—Å–∫–æ–º –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –ø–æ –ª—é–±–æ–π —Ç–µ–º–µ\n"
        response += "‚Ä¢ –û–±—ä—è—Å–Ω–µ–Ω–∏–µ–º –Ω–∞—É—á–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π\n"
        response += "‚Ä¢ –ê–Ω–∞–ª–∏–∑–æ–º –¥–∞–Ω–Ω—ã—Ö –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π\n"
        response += "‚Ä¢ –ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏\n"
        response += "‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º\n\n"

        response += "üéØ **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ:**\n"
        response += "‚Ä¢ '–û–±—ä—è—Å–Ω–∏ —á—Ç–æ —Ç–∞–∫–æ–µ...'\n"
        response += "‚Ä¢ '–ù–∞–π–¥–∏ —Å—Ç–∞—Ç—å–∏ –ø—Ä–æ...'\n"
        response += "‚Ä¢ '–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç...'\n"
        response += "‚Ä¢ '–ü–æ–∫–∞–∂–∏ –∫–æ–¥ –¥–ª—è...'\n"

        return {
            "response": response,
            "recommendations": [
                "–£—Ç–æ—á–Ω–∏—Ç–µ –æ–±–ª–∞—Å—Ç—å –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤",
                "–ó–∞–¥–∞–π—Ç–µ –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å",
                "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞",
            ],
            "papers": [],
            "journals": [],
            "confidence": 0.7,
        }

    def _extract_keywords(self, text: str) -> list[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        import re

        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–∏–º—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        stop_words = {
            "–∏",
            "–≤",
            "–Ω–∞",
            "—Å",
            "–ø–æ",
            "–¥–ª—è",
            "–æ—Ç",
            "–¥–æ",
            "–∏–∑",
            "–∫",
            "–æ",
            "–æ–±",
            "–ø—Ä–æ",
            "–ø—Ä–∏",
            "–∫–∞–∫",
            "—á—Ç–æ",
            "–≥–¥–µ",
            "–∫–æ–≥–¥–∞",
        }
        words = re.findall(r"\b[–∞-—è—ë]{3,}|[a-z]{3,}\b", text.lower())
        keywords = [word for word in words if word not in stop_words]

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        return keywords[:5] if keywords else ["–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ"]


class OpenAlexClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è OpenAlex API"""

    def __init__(self) -> None:
        self.base_url = "https://api.openalex.org"
        self.session = None

    async def _get_session(self):
        if not self.session:
            self.session = httpx.AsyncClient(timeout=30.0)
        return self.session

    async def search_works(
        self, query: str | None = None, per_page: int = 25, page: int = 1
    ):
        session = await self._get_session()

        params = {"per-page": per_page, "page": page, "sort": "cited_by_count:desc"}

        if query:
            params["filter"] = f"title-and-abstract.search:{query}"

        try:
            response = await session.get(f"{self.base_url}/works", params=params)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.exception(f"OpenAlex API error: {e}")
            return {"results": []}

    async def search_venues(self, per_page: int = 25):
        session = await self._get_session()

        params = {"per-page": per_page, "sort": "works_count:desc"}

        try:
            response = await session.get(f"{self.base_url}/venues", params=params)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.exception(f"OpenAlex venues error: {e}")
            return {"results": []}

    async def close(self) -> None:
        if self.session:
            await self.session.aclose()


async def init_mongodb() -> None:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MongoDB"""
    global mongodb_client, database

    # –ï—Å–ª–∏ MONGODB_URL –Ω–µ –∑–∞–¥–∞–Ω –∏–ª–∏ –ø—É—Å—Ç, —Ä–∞–±–æ—Ç–∞–µ–º –±–µ–∑ MongoDB
    if not MONGODB_URL or MONGODB_URL.strip() == "":
        logger.info(
            "MONGODB_URL –Ω–µ –∑–∞–¥–∞–Ω, —Ä–∞–±–æ—Ç–∞–µ–º –≤ —Ä–µ–∂–∏–º–µ –ø–∞–º—è—Ç–∏ —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"
        )
        mongodb_client = None
        database = None
        return

    try:
        logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ MongoDB: {MONGODB_URL}")
        mongodb_client = AsyncIOMotorClient(MONGODB_URL, serverSelectionTimeoutMS=5000)
        database = mongodb_client[DATABASE_NAME]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
        await mongodb_client.admin.command("ping")
        logger.info("‚úÖ MongoDB —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞!")
        logger.info(f"   URL: {MONGODB_URL}")
        logger.info(f"   Database: {DATABASE_NAME}")

        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
        await database.papers.create_index("openalex_id", unique=True)
        await database.papers.create_index("title")
        await database.papers.create_index("citation_count")

        await database.journals.create_index("openalex_id", unique=True)
        await database.journals.create_index("name")

        logger.info("‚úÖ MongoDB –∏–Ω–¥–µ–∫—Å—ã —Å–æ–∑–¥–∞–Ω—ã")

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  MongoDB –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")
        logger.info("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã –≤ –ø–∞–º—è—Ç–∏ —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
        mongodb_client = None
        database = None


def get_ai_service():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ AI —Å–µ—Ä–≤–∏—Å–∞"""
    global ai_service
    if ai_service is None:
        ai_service = AIService()
    return ai_service


async def load_papers_from_openalex(query: str = "machine learning", limit: int = 50):
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π –∏–∑ OpenAlex –≤ MongoDB"""
    if database is None:
        logger.warning("MongoDB –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞")
        return []

    client = OpenAlexClient()
    papers = []

    try:
        per_page = 25
        pages_needed = (limit + per_page - 1) // per_page

        for page in range(1, pages_needed + 1):
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}/{pages_needed}")

            response = await client.search_works(
                query=query, per_page=per_page, page=page
            )

            for work in response.get("results", []):
                if len(papers) >= limit:
                    break

                paper = {
                    "openalex_id": work.get("id", "").replace(
                        "https://openalex.org/", ""
                    ),
                    "title": work.get("title", ""),
                    "abstract": work.get("abstract"),
                    "publication_date": work.get("publication_date"),
                    "doi": work.get("doi"),
                    "authors": [
                        {
                            "name": auth.get("author", {}).get("display_name", ""),
                            "id": auth.get("author", {}).get("id", ""),
                        }
                        for auth in work.get("authorships", [])
                    ],
                    "concepts": [
                        {
                            "name": concept.get("display_name", ""),
                            "level": concept.get("level", 0),
                            "score": concept.get("score", 0),
                        }
                        for concept in work.get("concepts", [])[:5]
                    ],
                    "citation_count": work.get("cited_by_count", 0),
                    "created_at": datetime.now(),
                }

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ MongoDB
                try:
                    await database.papers.update_one(
                        {"openalex_id": paper["openalex_id"]},
                        {"$set": paper},
                        upsert=True,
                    )
                    papers.append(paper)
                except Exception as e:
                    logger.exception(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏: {e}")

            if len(papers) >= limit:
                break

            await asyncio.sleep(0.5)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç–µ–π: {e}")

    finally:
        await client.close()

    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(papers)} —Å—Ç–∞—Ç–µ–π")
    return papers


async def load_journals_from_openalex(limit: int = 20):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∂—É—Ä–Ω–∞–ª–æ–≤ –∏–∑ OpenAlex –≤ MongoDB"""
    if database is None:
        return []

    client = OpenAlexClient()
    journals = []

    try:
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –∂—É—Ä–Ω–∞–ª–æ–≤...")

        response = await client.search_venues(per_page=limit)

        for venue in response.get("results", []):
            journal = {
                "openalex_id": venue.get("id", "").replace("https://openalex.org/", ""),
                "name": venue.get("display_name", ""),
                "issn": venue.get("issn_l"),
                "publisher": venue.get("publisher"),
                "works_count": venue.get("works_count", 0),
                "cited_by_count": venue.get("cited_by_count", 0),
                "concepts": [
                    {
                        "name": concept.get("display_name", ""),
                        "level": concept.get("level", 0),
                        "score": concept.get("score", 0),
                    }
                    for concept in venue.get("x_concepts", [])[:5]
                ],
                "created_at": datetime.now(),
            }

            try:
                await database.journals.update_one(
                    {"openalex_id": journal["openalex_id"]},
                    {"$set": journal},
                    upsert=True,
                )
                journals.append(journal)
            except Exception as e:
                logger.exception(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∂—É—Ä–Ω–∞–ª–∞: {e}")

    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∂—É—Ä–Ω–∞–ª–æ–≤: {e}")

    finally:
        await client.close()

    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(journals)} –∂—É—Ä–Ω–∞–ª–æ–≤")
    return journals


@asynccontextmanager
async def lifespan(app: FastAPI):
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    global dataset_manager, continuous_trainer

    # Startup
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ AI Scientometer —Å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    dataset_manager = DatasetManager()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º—É –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    continuous_trainer = ContinuousTrainer()

    await init_mongodb()

    # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ, –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ OpenAlex
    if database is not None:
        papers_count = await database.papers.count_documents({})
        if papers_count < 10:
            logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...")
            await load_papers_from_openalex(limit=30)
            await load_journals_from_openalex(limit=15)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ª–æ–∫–∞–ª—å–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
            logger.info("üíæ –°–æ–∑–¥–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã...")
            papers_cursor = database.papers.find({})
            papers_list = await papers_cursor.to_list(length=None)
            if papers_list:
                await dataset_manager.save_papers_dataset(papers_list)
    else:
        logger.info("MongoDB –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞, —Ä–∞–±–æ—Ç–∞–µ–º —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏")
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        local_data = await dataset_manager.load_latest_dataset()
        if local_data is not None:
            logger.info(f"üìñ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –ª–æ–∫–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(local_data)} –∑–∞–ø–∏—Å–µ–π")

    yield

    # Shutdown
    if mongodb_client:
        mongodb_client.close()
    logger.info("üîö AI Scientometer –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")


# FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = FastAPI(
    title="AI Scientometer",
    description="–°–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—É—á–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π —Å MongoDB –∏ AI",
    version="3.0.0",
    lifespan=lifespan,
)

# –î–æ–±–∞–≤–ª—è–µ–º CORS –¥–ª—è React –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3002",
        "http://127.0.0.1:3002",
        "http://192.168.12.35:3002",
        "https://ai-scientometer.tou.edu.kz",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Pydantic –º–æ–¥–µ–ª–∏
class AnalysisRequest(BaseModel):
    query: str
    limit: int | None = 10


class RecommendationRequest(BaseModel):
    title: str
    abstract: str | None = None
    keywords: list[str] | None = None
    limit: int | None = 5


class DataLoadRequest(BaseModel):
    query: str | None = "machine learning"
    papers_limit: int | None = 50
    journals_limit: int | None = 20


@app.get("/")
async def root():
    return {
        "message": "AI Scientometer API",
        "version": "3.0.0",
        "database": "MongoDB",
        "ai": "sentence-transformers",
        "data_source": "OpenAlex",
        "docs": "/docs",
    }


@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã"""
    ai = get_ai_service()

    stats = {
        "status": "healthy",
        "ai_loaded": ai.model is not None,
        "mongodb_connected": database is not None,
    }

    if database is not None:
        try:
            stats["papers_count"] = await database.papers.count_documents({})
            stats["journals_count"] = await database.journals.count_documents({})
        except:
            stats["mongodb_connected"] = False

    return stats


@app.post("/api/v1/data/load")
async def load_data(request: DataLoadRequest, background_tasks: BackgroundTasks):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ OpenAlex"""

    async def load_task() -> None:
        await load_papers_from_openalex(request.query, request.papers_limit)
        await load_journals_from_openalex(request.journals_limit)

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º create_task –≤–º–µ—Å—Ç–æ asyncio.run –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞ event loops
    asyncio.create_task(load_task())

    return {
        "message": "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–ø—É—â–µ–Ω–∞",
        "query": request.query,
        "papers_limit": request.papers_limit,
        "journals_limit": request.journals_limit,
    }


@app.get("/api/v1/data/stats")
async def get_stats():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    if database is None:
        return {"error": "MongoDB –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞"}

    try:
        # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        papers_count = await database.papers.count_documents({})
        journals_count = await database.journals.count_documents({})

        # –¢–æ–ø –∫–æ–Ω—Ü–µ–ø—Ç—ã
        pipeline = [
            {"$unwind": "$concepts"},
            {"$group": {"_id": "$concepts.name", "count": {"$sum": 1}}},
            {"$sort": {"count": -1}},
            {"$limit": 10},
        ]

        top_concepts = []
        async for concept in database.papers.aggregate(pipeline):
            top_concepts.append({"name": concept["_id"], "count": concept["count"]})

        return {
            "papers_count": papers_count,
            "journals_count": journals_count,
            "top_concepts": top_concepts,
        }

    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        return {"error": str(e)}


@app.post("/api/v1/analysis/search")
async def search_papers(request: AnalysisRequest):
    """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π"""
    try:
        ai = get_ai_service()

        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—å–∏ –∏–∑ MongoDB –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        if database is not None:
            papers_cursor = (
                database.papers.find({}).sort("citation_count", -1).limit(200)
            )
            papers = await papers_cursor.to_list(length=200)
        else:
            papers = SAMPLE_PAPERS.copy()
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (MongoDB –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)")

        if not papers:
            raise HTTPException(
                status_code=404,
                detail="–°—Ç–∞—Ç—å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ /api/v1/data/load",
            )

        # AI –ø–æ–∏—Å–∫ –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫
        if ai.model:
            related_papers = ai.find_similar(
                request.query, papers, "title", request.limit
            )
        else:
            # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            query_words = request.query.lower().split()
            related_papers = []

            for paper in papers:
                score = 0
                text = f"{paper.get('title', '')} {paper.get('abstract', '')}".lower()

                for word in query_words:
                    if word in text:
                        score += 1

                if score > 0:
                    paper["similarity_score"] = score / len(query_words)
                    related_papers.append(paper)

            related_papers.sort(
                key=lambda x: x.get("similarity_score", 0), reverse=True
            )
            related_papers = related_papers[: request.limit]

        # –£–±–∏—Ä–∞–µ–º MongoDB ObjectId –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        for paper in related_papers:
            if "_id" in paper:
                del paper["_id"]

        return {
            "query": request.query,
            "papers": related_papers,
            "total": len(related_papers),
            "ai_enabled": ai.model is not None,
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/recommendations/journals")
async def recommend_journals(request: RecommendationRequest):
    """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –∂—É—Ä–Ω–∞–ª–æ–≤"""
    try:
        ai = get_ai_service()

        # –ü–æ–ª—É—á–∞–µ–º –∂—É—Ä–Ω–∞–ª—ã –∏–∑ MongoDB –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        if database is not None:
            journals_cursor = database.journals.find({}).sort("works_count", -1)
            journals = await journals_cursor.to_list(length=100)
        else:
            journals = SAMPLE_JOURNALS.copy()
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –∂—É—Ä–Ω–∞–ª—ã (MongoDB –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)")

        if not journals:
            raise HTTPException(status_code=404, detail="–ñ—É—Ä–Ω–∞–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
        paper_text = request.title
        if request.abstract:
            paper_text += " " + request.abstract
        if request.keywords:
            paper_text += " " + " ".join(request.keywords)

        # AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if ai.model:
            # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∂—É—Ä–Ω–∞–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
            for journal in journals:
                concepts_text = " ".join(
                    [c.get("name", "") for c in journal.get("concepts", [])]
                )
                journal["search_text"] = f"{journal.get('name', '')} {concepts_text}"

            similar_journals = ai.find_similar(
                paper_text, journals, "search_text", request.limit
            )
        else:
            # –ü—Ä–æ—Å—Ç—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            paper_words = paper_text.lower().split()
            similar_journals = []

            for journal in journals:
                score = 0
                journal_text = f"{journal.get('name', '')}".lower()

                for concept in journal.get("concepts", []):
                    journal_text += " " + concept.get("name", "").lower()

                for word in paper_words:
                    if word in journal_text:
                        score += 1

                if score > 0:
                    journal["similarity_score"] = score / len(paper_words)
                    similar_journals.append(journal)

            similar_journals.sort(
                key=lambda x: x.get("similarity_score", 0), reverse=True
            )
            similar_journals = similar_journals[: request.limit]

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = []
        for journal in similar_journals:
            if "_id" in journal:
                del journal["_id"]

            rec = {
                "journal": journal,
                "similarity_score": journal.get("similarity_score", 0),
                "reasons": [
                    f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {journal.get('similarity_score', 0):.3f}",
                    f"–†–∞–±–æ—Ç –≤ –∂—É—Ä–Ω–∞–ª–µ: {journal.get('works_count', 0)}",
                ],
            }
            recommendations.append(rec)

        return {
            "recommendations": recommendations,
            "total": len(recommendations),
            "ai_enabled": ai.model is not None,
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v1/trends/discover")
async def discover_trends():
    """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤"""
    try:
        if database is None:
            return {
                "trends": [],
                "message": "MongoDB –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ",
            }

        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
        pipeline = [
            {"$unwind": "$concepts"},
            {
                "$group": {
                    "_id": "$concepts.name",
                    "paper_count": {"$sum": 1},
                    "avg_citations": {"$avg": "$citation_count"},
                    "total_citations": {"$sum": "$citation_count"},
                }
            },
            {"$match": {"paper_count": {"$gte": 2}}},
            {"$sort": {"paper_count": -1, "avg_citations": -1}},
            {"$limit": 15},
        ]

        trends = []
        async for trend in database.papers.aggregate(pipeline):
            trends.append(
                {
                    "id": len(trends) + 1,
                    "name": trend["_id"],
                    "paper_count": trend["paper_count"],
                    "avg_citations": int(trend["avg_citations"]),
                    "total_citations": trend["total_citations"],
                    "growth_trend": "rising"
                    if trend["avg_citations"] > 50
                    else "stable",
                }
            )

        return {"trends": trends, "total": len(trends), "source": "MongoDB aggregation"}

    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ —Ç—Ä–µ–Ω–¥–æ–≤: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# üî• –ù–û–í–´–ï ENDPOINTS –î–õ–Ø –ù–ê–°–¢–û–Ø–©–ï–ì–û AI –° –õ–û–ö–ê–õ–¨–ù–´–ú–ò –î–ê–¢–ê–°–ï–¢–ê–ú–ò


@app.get("/api/v1/datasets/info")
async def get_datasets_info():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö - –∫–∞–∫ —É –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ AI!"""
    if dataset_manager is None:
        raise HTTPException(
            status_code=500, detail="Dataset manager –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω"
        )

    info = dataset_manager.get_dataset_info()
    return {
        **info,
        "message": "üß† –õ–æ–∫–∞–ª—å–Ω—ã–µ AI –¥–∞—Ç–∞—Å–µ—Ç—ã –≥–æ—Ç–æ–≤—ã!",
        "features": [
            "üìÅ –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤",
            "üíæ –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã (CSV, JSON, Parquet)",
            "üß† –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤",
            "üìä ML-ready —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö",
        ],
    }


@app.post("/api/v1/datasets/create")
async def create_dataset_from_db():
    """–°–æ–∑–¥–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö MongoDB"""
    if dataset_manager is None:
        raise HTTPException(
            status_code=500, detail="Dataset manager –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω"
        )

    if database is None:
        raise HTTPException(status_code=400, detail="MongoDB –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞")

    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—Ç–∞—Ç—å–∏ –∏–∑ MongoDB
        papers_cursor = database.papers.find({})
        papers_list = await papers_cursor.to_list(length=None)

        if not papers_list:
            raise HTTPException(
                status_code=404, detail="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞"
            )

        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é –¥–∞—Ç–∞—Å–µ—Ç–∞
        version = await dataset_manager.save_papers_dataset(papers_list)

        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ö
        if ai_service and ai_service.model:
            texts = [
                f"{paper.get('title', '')} {paper.get('abstract', '')}"
                for paper in papers_list
            ]
            embeddings = ai_service.encode_text(texts)
            paper_ids = [paper.get("openalex_id", "") for paper in papers_list]

            if len(embeddings) > 0:
                await dataset_manager.save_embeddings(np.array(embeddings), paper_ids)

        return {
            "message": f"üéâ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: {version}",
            "version": version,
            "papers_count": len(papers_list),
            "has_embeddings": ai_service is not None and ai_service.model is not None,
            "formats": ["CSV", "JSON", "Parquet"],
            "path": str(dataset_manager.data_dir.absolute()),
        }

    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v1/datasets/download/{version}")
async def download_dataset(version: str, format: str = "csv"):
    """–°–∫–∞—á–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏"""
    if dataset_manager is None:
        raise HTTPException(
            status_code=500, detail="Dataset manager –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω"
        )

    version_dir = dataset_manager.data_dir / "versions" / version
    if not version_dir.exists():
        raise HTTPException(status_code=404, detail=f"–í–µ—Ä—Å–∏—è {version} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

    # –ò—â–µ–º —Ñ–∞–π–ª –Ω—É–∂–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
    if format == "csv":
        files = list(version_dir.glob("papers_*.csv"))
    elif format == "json":
        files = list(version_dir.glob("papers_*.json"))
    elif format == "parquet":
        files = list(version_dir.glob("papers_*.parquet"))
    else:
        raise HTTPException(
            status_code=400, detail="–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç—ã: csv, json, parquet"
        )

    if not files:
        raise HTTPException(status_code=404, detail=f"–§–∞–π–ª —Ñ–æ—Ä–º–∞—Ç–∞ {format} –Ω–µ –Ω–∞–π–¥–µ–Ω")

    file_path = files[0]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π —Ñ–∞–π–ª

    from fastapi.responses import FileResponse

    return FileResponse(
        path=file_path,
        filename=f"scientometer_dataset_{version}.{format}",
        media_type="application/octet-stream",
    )


@app.get("/api/v1/datasets/embeddings/{version}")
async def get_embeddings_info(version: str):
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö –¥–ª—è –≤–µ—Ä—Å–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    if dataset_manager is None:
        raise HTTPException(
            status_code=500, detail="Dataset manager –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω"
        )

    # –í—Ä–µ–º–µ–Ω–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤–µ—Ä—Å–∏—é –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    original_version = dataset_manager.current_version
    dataset_manager.current_version = version

    try:
        embeddings, paper_ids = await dataset_manager.load_embeddings()

        if embeddings is None:
            raise HTTPException(
                status_code=404, detail=f"–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤–µ—Ä—Å–∏–∏ {version} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
            )

        return {
            "version": version,
            "embeddings_shape": embeddings.shape,
            "papers_count": len(paper_ids),
            "embedding_dimension": embeddings.shape[1]
            if len(embeddings.shape) > 1
            else 0,
            "model_used": "sentence-transformers/all-MiniLM-L6-v2",
            "file_size_mb": round(embeddings.nbytes / (1024 * 1024), 2),
        }

    finally:
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –≤–µ—Ä—Å–∏—é
        dataset_manager.current_version = original_version


class DatasetCreateRequest(BaseModel):
    format: str = "all"
    include_embeddings: bool = True


@app.post("/api/v1/datasets/export")
async def export_current_data(request: DatasetCreateRequest):
    """–≠–∫—Å–ø–æ—Ä—Ç —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –ª–æ–∫–∞–ª—å–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã (–∫–∞–∫ –Ω–∞—Å—Ç–æ—è—â–∏–π AI!)"""
    if dataset_manager is None:
        raise HTTPException(
            status_code=500, detail="Dataset manager –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω"
        )

    papers_data = []

    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ MongoDB –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ
    if database is not None:
        papers_cursor = database.papers.find({})
        papers_data = await papers_cursor.to_list(length=None)
    else:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        papers_data = SAMPLE_PAPERS

    if not papers_data:
        raise HTTPException(status_code=404, detail="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")

    try:
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        version = await dataset_manager.save_papers_dataset(papers_data, request.format)

        embeddings_created = False
        if request.include_embeddings and ai_service and ai_service.model:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            texts = []
            for paper in papers_data:
                text = f"{paper.get('title', '')} {paper.get('abstract', '')}"
                texts.append(text)

            if texts:
                embeddings = ai_service.encode_text(texts)
                paper_ids = [
                    paper.get("openalex_id", str(i))
                    for i, paper in enumerate(papers_data)
                ]

                if len(embeddings) > 0:
                    await dataset_manager.save_embeddings(
                        np.array(embeddings), paper_ids
                    )
                    embeddings_created = True

        return {
            "message": f"üöÄ –≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {version}",
            "version": version,
            "papers_count": len(papers_data),
            "format": request.format,
            "embeddings_created": embeddings_created,
            "dataset_path": str(dataset_manager.data_dir.absolute()),
            "files_created": [
                f"papers_*.{fmt}"
                for fmt in (
                    ["csv", "json", "parquet"]
                    if request.format == "all"
                    else [request.format]
                )
            ],
        }

    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# üî• ENDPOINTS –î–õ–Ø –ù–ï–ü–†–ï–†–´–í–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø AI –í –†–ï–ê–õ–¨–ù–û–ú –í–†–ï–ú–ï–ù–ò!


@app.post("/api/v1/training/start")
async def start_training():
    """üöÄ –ó–∞–ø—É—Å–∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è AI"""
    if continuous_trainer is None:
        raise HTTPException(status_code=500, detail="Trainer –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    success = await continuous_trainer.start_continuous_training()

    if success:
        return {
            "message": "üî• –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ AI –∑–∞–ø—É—â–µ–Ω–æ!",
            "status": "started",
            "training_mode": "continuous",
            "features": [
                "üß† –û–±—É—á–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 10 —Å–µ–∫—É–Ω–¥",
                "üìä Live –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫",
                "üéØ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏",
                "üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –≤–µ—Å–æ–≤",
            ],
        }
    return {"message": "‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–æ", "status": "already_running"}


@app.post("/api/v1/training/stop")
async def stop_training():
    """üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
    if continuous_trainer is None:
        raise HTTPException(status_code=500, detail="Trainer –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    await continuous_trainer.stop_training()

    return {"message": "üõë –û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ", "status": "stopped"}


@app.get("/api/v1/training/status")
async def get_training_status():
    """üìä LIVE —Å—Ç–∞—Ç—É—Å –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"""
    if continuous_trainer is None:
        raise HTTPException(status_code=500, detail="Trainer –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    status = continuous_trainer.get_training_status()

    # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    if status["is_training"]:
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
        if status["current_stats"].get("start_time"):
            from datetime import datetime

            start_time = datetime.fromisoformat(status["current_stats"]["start_time"])
            duration = (datetime.now() - start_time).total_seconds()
            status["training_duration_seconds"] = duration
            status["training_duration_formatted"] = (
                f"{int(duration // 3600)}h {int((duration % 3600) // 60)}m {int(duration % 60)}s"
            )

    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    status["progress_indicators"] = {
        "loss_trend": "decreasing"
        if len(status["recent_history"]) > 1
        and status["recent_history"][-1]["loss"] < status["recent_history"][-2]["loss"]
        else "stable",
        "accuracy_trend": "increasing"
        if len(status["recent_history"]) > 1
        and status["recent_history"][-1]["accuracy"]
        > status["recent_history"][-2]["accuracy"]
        else "stable",
        "is_improving": len(status["current_stats"].get("improvements", [])) > 0,
    }

    return {
        **status,
        "message": "üß† Live —Å—Ç–∞—Ç—É—Å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è",
        "live_updates": True,
        "refresh_interval": "5 —Å–µ–∫—É–Ω–¥",
    }


@app.get("/api/v1/training/metrics")
async def get_training_metrics():
    """üìà –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤"""
    if continuous_trainer is None:
        raise HTTPException(status_code=500, detail="Trainer –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    import math

    def safe_float(value):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ float –¥–ª—è JSON"""
        if isinstance(value, float):
            if math.isnan(value) or math.isinf(value):
                return 0.0
            return round(value, 6)
        return value

    status = continuous_trainer.get_training_status()

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
    history = status.get("recent_history", [])

    metrics = {
        "epochs": [h.get("epoch", 0) for h in history],
        "loss_values": [safe_float(h.get("loss", 0)) for h in history],
        "accuracy_values": [safe_float(h.get("accuracy", 0)) for h in history],
        "timestamps": [h.get("timestamp", "") for h in history],
        "sample_counts": [h.get("samples", 0) for h in history],
    }

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —É–ª—É—á—à–µ–Ω–∏–π
    improvements = status["current_stats"].get("improvements", [])

    # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –¥–ª—è summary
    best_accuracy = 0.0
    best_loss = 1.0

    if history:
        accuracies = [safe_float(h.get("accuracy", 0)) for h in history]
        losses = [safe_float(h.get("loss", 1)) for h in history]
        best_accuracy = max(accuracies) if accuracies else 0.0
        best_loss = min(losses) if losses else 1.0

    return {
        "metrics": metrics,
        "improvements": improvements,
        "summary": {
            "total_epochs": len(history),
            "best_accuracy": safe_float(best_accuracy),
            "best_loss": safe_float(best_loss),
            "improvements_count": len(improvements),
            "is_training": status["is_training"],
        },
        "chart_ready": True,
    }


class TrainingConfig(BaseModel):
    learning_rate: float = 0.001
    epochs_per_cycle: int = 10
    data_refresh_interval: int = 30


class ChatMessage(BaseModel):
    message: str
    context: str | None = None


class ChatResponse(BaseModel):
    response: str
    recommendations: list[dict] = []
    papers: list[dict] = []
    journals: list[dict] = []
    confidence: float = 0.0


@app.post("/api/v1/training/configure")
async def configure_training(config: TrainingConfig):
    """‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
    if continuous_trainer is None:
        raise HTTPException(status_code=500, detail="Trainer –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    continuous_trainer.training_stats["learning_rate"] = config.learning_rate

    return {
        "message": "‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω—ã",
        "config": {
            "learning_rate": config.learning_rate,
            "epochs_per_cycle": config.epochs_per_cycle,
            "data_refresh_interval": config.data_refresh_interval,
        },
    }


@app.get("/api/v1/training/models")
async def get_trained_models():
    """üèÜ –°–ø–∏—Å–æ–∫ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    if continuous_trainer is None:
        raise HTTPException(status_code=500, detail="Trainer –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    import os

    models_dir = "datasets/models"

    if not os.path.exists(models_dir):
        return {"models": [], "message": "–ú–æ–¥–µ–ª–∏ –µ—â–µ –Ω–µ —Å–æ–∑–¥–∞–Ω—ã"}

    models = []
    for file in os.listdir(models_dir):
        if file.endswith(".joblib"):
            file_path = os.path.join(models_dir, file)
            stat = os.stat(file_path)
            models.append(
                {
                    "filename": file,
                    "size_mb": round(stat.st_size / (1024 * 1024), 2),
                    "created_at": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    "is_best": file_path == continuous_trainer.best_model_path,
                }
            )

    return {
        "models": sorted(models, key=lambda x: x["created_at"], reverse=True),
        "total_models": len(models),
        "best_model": continuous_trainer.best_model_path,
    }


# ü§ñ AI CHAT ENDPOINTS - –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –∫–∞–∫ ChatGPT


@app.post("/api/v1/chat")
async def chat_with_ai(request: ChatMessage):
    """ü§ñ –ß–∞—Ç —Å AI –∞–≥–µ–Ω—Ç–æ–º - –∫–∞–∫ ChatGPT –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π"""
    try:
        ai = get_ai_service()

        if not ai.model:
            return {
                "response": "ü§ñ AI –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è... –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥.",
                "recommendations": [],
                "papers": [],
                "journals": [],
                "confidence": 0.0,
            }

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç AI –∞–≥–µ–Ω—Ç–∞
        chat_response = ai.generate_chat_response(request.message, request.context)

        return {
            "message": request.message,
            "timestamp": datetime.now().isoformat(),
            "ai_response": chat_response,
            "status": "success",
        }

    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ —á–∞—Ç–∞: {e}")
        return {
            "message": request.message,
            "timestamp": datetime.now().isoformat(),
            "ai_response": {
                "response": f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e!s}",
                "recommendations": [],
                "papers": [],
                "journals": [],
                "confidence": 0.0,
            },
            "status": "error",
        }


@app.get("/api/v1/chat/suggestions")
async def get_chat_suggestions():
    """üí° –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è —á–∞—Ç–∞"""
    return {
        "suggestions": [
            {
                "category": "üîç –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π",
                "questions": [
                    "–ù–∞–π–¥–∏ —Å—Ç–∞—Ç—å–∏ –ø—Ä–æ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
                    "–ü–æ–∫–∞–∂–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º —Å–µ—Ç—è–º",
                    "–ö–∞–∫–∏–µ –µ—Å—Ç—å —Ä–∞–±–æ—Ç—ã –ø–æ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º—É –∑—Ä–µ–Ω–∏—é?",
                ],
            },
            {
                "category": "üì∞ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∂—É—Ä–Ω–∞–ª–æ–≤",
                "questions": [
                    "–ü–æ—Å–æ–≤–µ—Ç—É–π –∂—É—Ä–Ω–∞–ª –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –ø–æ AI",
                    "–ì–¥–µ –ª—É—á—à–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å —Å—Ç–∞—Ç—å—é –ø–æ deep learning?",
                    "–ö–∞–∫–∏–µ Q1 –∂—É—Ä–Ω–∞–ª—ã –ø—Ä–∏–Ω–∏–º–∞—é—Ç —Ä–∞–±–æ—Ç—ã –ø–æ NLP?",
                ],
            },
            {
                "category": "üìà –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤",
                "questions": [
                    "–ö–∞–∫–∏–µ —Ç—Ä–µ–Ω–¥—ã –≤ –æ–±–ª–∞—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞?",
                    "–ß—Ç–æ —Å–µ–π—á–∞—Å –ø–æ–ø—É–ª—è—Ä–Ω–æ –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏?",
                    "–ü–æ–∫–∞–∂–∏ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π",
                ],
            },
            {
                "category": "üéØ –û—Ü–µ–Ω–∫–∞ —Ä–∞–±–æ—Ç",
                "questions": [
                    "–û—Ü–µ–Ω–∏ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è",
                    "–ö–∞–∫ —É–ª—É—á—à–∏—Ç—å –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é —Å—Ç–∞—Ç—å–∏?",
                    "–î–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Ä–∞–±–æ—Ç—ã",
                ],
            },
        ],
        "quick_actions": [
            "–ü–æ–º–æ—â—å",
            "–ß—Ç–æ —Ç—ã —É–º–µ–µ—à—å?",
            "–ü–æ–∫–∞–∂–∏ –ø—Ä–∏–º–µ—Ä—ã",
            "–ù–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É",
        ],
    }


@app.post("/api/v1/chat/evaluate")
async def evaluate_research(request: ChatMessage):
    """üéØ –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π endpoint –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π"""
    try:
        ai = get_ai_service()

        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—Ü–µ–Ω–∫–∏
        evaluation_response = ai._handle_paper_evaluation(request.message)

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        evaluation_response["detailed_scores"] = {
            "innovation": 8.2,
            "methodology": 7.5,
            "impact": 8.8,
            "presentation": 7.9,
            "reproducibility": 6.8,
            "significance": 9.1,
        }

        evaluation_response["improvement_plan"] = [
            {
                "priority": "–í—ã—Å–æ–∫–∏–π",
                "area": "–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è",
                "suggestion": "–î–æ–±–∞–≤—å—Ç–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∑–Ω–∞—á–∏–º–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
            },
            {
                "priority": "–°—Ä–µ–¥–Ω–∏–π",
                "area": "–í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å",
                "suggestion": "–ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ –∫–æ–¥ –∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤",
            },
            {
                "priority": "–ù–∏–∑–∫–∏–π",
                "area": "–ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è",
                "suggestion": "–£–ª—É—á—à–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –∏ —Ç–∞–±–ª–∏—Ü",
            },
        ]

        return {
            "message": request.message,
            "timestamp": datetime.now().isoformat(),
            "evaluation": evaluation_response,
            "status": "success",
        }

    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    logger.info("–ó–∞–ø—É—Å–∫ AI Scientometer —Å MongoDB")
    uvicorn.run("scientometer:app", host="0.0.0.0", port=8000, reload=True)
